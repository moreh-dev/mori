# Copyright Â© Advanced Micro Devices, Inc. All rights reserved.
#
# MIT License
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
import mori
from tests.python.ops.test_dispatch_combine import EpDispatchCombineTestCase
from tests.python.utils import TorchDistContext, get_free_port
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F


class SharedExperts(nn.Module):

    def __init__(self, ffn_dim, hidden_dim, device, dtype=torch.bfloat16):
        super().__init__()
        self.ffn_dim = ffn_dim
        self.gate_and_up_proj = torch.nn.Linear(
            hidden_dim, ffn_dim * 2, bias=False, dtype=dtype, device=device
        )
        self.down_proj = torch.nn.Linear(
            ffn_dim, hidden_dim, bias=False, dtype=dtype, device=device
        )

    def forward(self, hidden_states):
        gate_up_output = self.gate_and_up_proj(hidden_states)
        x = F.silu(
            gate_up_output[..., : self.ffn_dim] * gate_up_output[..., self.ffn_dim :]
        )
        return self.down_proj(x)


class EpDispatchCombineBenchmark(EpDispatchCombineTestCase):
    def __init__(self, config, profile_dir=None):
        super().__init__(config)
        self.profile = profile_dir is not None
        self.compute_stream = torch.cuda.Stream()

        if self.profile:
            self.profiler = torch.profiler.profile(
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA,
                ],
                on_trace_ready=torch.profiler.tensorboard_trace_handler(
                    profile_dir, use_gzip=True
                ),
            )
        self.shared_experts = SharedExperts(
            ffn_dim=2048, hidden_dim=7168, device=self.device
        )

    def gen_test_data(self):
        return super().gen_test_data(use_max_token_num=True)

    def run_once(self, op, test_data, check_result):
        (
            all_rank_num_token,
            all_rank_indices,
            all_rank_input,
            all_rank_weights,
            all_rank_scales,
        ) = test_data

        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)

        self.sync()
        start_event.record()
        (
            dispatch_output,
            dispatch_weights,
            dispatch_scales,
            dispatch_indices,
            dispatch_recv_num_token,
        ) = op.dispatch(
            all_rank_input[self.config.rank],
            all_rank_weights[self.config.rank],
            # None,
            all_rank_scales[self.config.rank],
            all_rank_indices[self.config.rank],
            warp_per_block=16,
            do_send=True,
            do_recv=False,
        )

        with torch.cuda.stream(self.compute_stream):
            _ = self.shared_experts(all_rank_input[self.config.rank])

        torch.cuda.current_stream().wait_stream(self.compute_stream)

        _ = op.dispatch(
            all_rank_input[self.config.rank],
            all_rank_weights[self.config.rank],
            # None,
            all_rank_scales[self.config.rank],
            all_rank_indices[self.config.rank],
            warp_per_block=16,
            do_send=False,
            do_recv=True,
        )

        end_event.record()
        self.sync()
        disp_duration = start_event.elapsed_time(end_event)

        if check_result:
            self.check_dispatch_result(
                op,
                test_data,
                dispatch_output,
                dispatch_weights,
                dispatch_scales,
                dispatch_indices,
                dispatch_recv_num_token,
            )

        total_recv_num_token = dispatch_recv_num_token[0].item()

        combine_input = op.get_registered_combine_input_buffer(self.config.data_type)
        combine_input[:total_recv_num_token, :].copy_(
            dispatch_output[:total_recv_num_token, :]
        )

        self.sync()
        start_event.record()
        combine_output, _ = op.combine(
            combine_input,
            # dispatch_weights,
            None,
            dispatch_indices,
            # block_num=128,
            warp_per_block=4,
        )
        end_event.record()
        self.sync()
        comb_duration = start_event.elapsed_time(end_event)

        if check_result:
            self.check_combine_result(op, test_data, combine_output)
        op.reset()
        self.sync()

        element_size = all_rank_input[self.config.rank].element_size()
        total_bytes = total_recv_num_token * self.config.hidden_dim * element_size
        ll_mode_scale = (
            self.config.max_num_inp_token_per_rank
            * self.config.num_experts_per_token
            / total_recv_num_token
        )
        disp_bandwidth = total_bytes / (1000**3) / (disp_duration / (10**3))
        comb_bandwidth = total_bytes / (1000**3) / (comb_duration / (10**3))

        return (
            disp_duration,
            comb_duration,
            disp_bandwidth,
            comb_bandwidth,
            total_bytes,
            ll_mode_scale,
        )

    def run(self, op, warmup=1, iters=10):
        test_data = self.gen_test_data()
        for _ in range(warmup):
            self.run_once(op, test_data, False)

        disp_duration_us_list = []
        disp_bandwidth_GB_list = []
        comb_duration_us_list = []
        comb_bandwidth_GB_list = []
        avg_total_bytes_MB_list = []

        test_data_list = [self.gen_test_data() for i in range(iters)]

        for i in range(iters):

            if self.profile and i == iters - 1:
                self.profiler.start()
            self.sync()

            disp_dur, comb_dur, disp_bw, comb_bw, total_bytes, ll_mode_scale = (
                self.run_once(op, test_data_list[i], False)
            )
            if self.profile and i == iters - 1:
                self.profiler.stop()
                print(
                    self.profiler.key_averages().table(sort_by="self_cuda_time_total")
                )

            disp_dur_list = [torch.zeros(1) for _ in range(self.config.world_size)]
            disp_bw_list = [torch.zeros(1) for _ in range(self.config.world_size)]
            comb_dur_list = [torch.zeros(1) for _ in range(self.config.world_size)]
            comb_bw_list = [torch.zeros(1) for _ in range(self.config.world_size)]
            total_bytes_list = [torch.zeros(1) for _ in range(self.config.world_size)]

            dist.all_gather(disp_dur_list, torch.tensor([disp_dur * 1000]))
            dist.all_gather(disp_bw_list, torch.tensor([disp_bw]))
            dist.all_gather(comb_dur_list, torch.tensor([comb_dur * 1000]))
            dist.all_gather(comb_bw_list, torch.tensor([comb_bw]))
            dist.all_gather(total_bytes_list, torch.tensor([total_bytes / (1024**2)]))

            disp_duration_us_list.append([int(t.item()) for t in disp_dur_list])
            disp_bandwidth_GB_list.append([int(t.item()) for t in disp_bw_list])
            comb_duration_us_list.append([int(t.item()) for t in comb_dur_list])
            comb_bandwidth_GB_list.append([int(t.item()) for t in comb_bw_list])
            avg_total_bytes_MB_list.append(
                int(torch.tensor(total_bytes_list).mean().item())
            )

        if self.config.rank == 0:
            print("Dispatch result:")
            for i, duration_us in enumerate(disp_duration_us_list):
                algo_bw = sum(disp_bandwidth_GB_list[i]) / self.config.world_size
                bus_bw = int(  # noqa: F841
                    algo_bw * (self.config.world_size - 1) / self.config.world_size
                )
                print(
                    f"Round {i} duration(us) {duration_us} "
                    f"bandwidth(GB/s) {disp_bandwidth_GB_list[i]}"
                    f"avg bytes(MB) {avg_total_bytes_MB_list[i]} bw {algo_bw} / {algo_bw*ll_mode_scale:.2f}"
                )

            print()
            print("Combine result:")
            for i, duration_us in enumerate(comb_duration_us_list):
                algo_bw = sum(comb_bandwidth_GB_list[i]) / self.config.world_size
                bus_bw = int(  # noqa: F841
                    algo_bw * (self.config.world_size - 1) / self.config.world_size
                )
                print(
                    f"Round {i} duration(us) {duration_us} "
                    f"bandwidth(GB/s) {comb_bandwidth_GB_list[i]}"
                    f"avg bytes(MB) {avg_total_bytes_MB_list[i]} bw {algo_bw} / {algo_bw*ll_mode_scale:.2f}"
                )

    def stress_once(self, op, test_data):
        (
            all_rank_num_token,
            all_rank_indices,
            all_rank_input,
            all_rank_weights,
            all_rank_scales,
        ) = test_data

        (
            dispatch_output,
            dispatch_weights,
            dispatch_scales,
            dispatch_indices,
            dispatch_recv_num_token,
        ) = op.dispatch(
            all_rank_input[self.config.rank],
            all_rank_weights[self.config.rank],
            # None,
            all_rank_scales[self.config.rank],
            all_rank_indices[self.config.rank],
            block_num=80,
            warp_per_block=16,
        )

        combine_output, _ = op.combine(
            dispatch_output,
            None,
            dispatch_indices,
            block_num=80,
            warp_per_block=16,
        )
        torch.cuda.synchronize()

    def stress(self, op):
        test_data_list = [self.gen_test_data() for i in range(5)]
        for i in range(100):
            if self.config.rank == 0:
                print(f"Round {i} begin")
            self.stress_once(op, test_data_list[i % 5])

    def stress_graph(self, op):
        test_data = self.gen_test_data()
        g = torch.cuda.CUDAGraph()
        with torch.cuda.graph(g):
            (
                all_rank_num_token,
                all_rank_indices,
                all_rank_input,
                all_rank_weights,
                all_rank_scales,
            ) = test_data

            (
                dispatch_output,
                dispatch_weights,
                dispatch_scales,
                dispatch_indices,
                dispatch_recv_num_token,
            ) = op.dispatch(
                all_rank_input[self.config.rank],
                all_rank_weights[self.config.rank],
                # None,
                all_rank_scales[self.config.rank],
                all_rank_indices[self.config.rank],
                block_num=80,
                warp_per_block=16,
            )

            combine_output, _ = op.combine(
                dispatch_output,
                None,
                dispatch_indices,
                block_num=80,
                warp_per_block=16,
            )
        torch.cuda.synchronize()
        for i in range(135):
            if self.config.rank == 0:
                print(f"Round {i} begin")
            g.replay()
            torch.cuda.synchronize()


def _bench_dispatch_combine(
    rank,
    world_size,
    port,
    max_num_inp_token_per_rank=128,
    block_num=80,
    profile_dir=None,
    data_type=torch.bfloat16,
    # data_type=torch.float8_e4m3fnuz,
    hidden_dim=7168,
    scale_dim=0,
    scale_type_size=0,
    num_experts_per_rank=32,
    num_experts_per_token=8,
):
    config = mori.ops.EpDispatchCombineConfig(
        data_type=data_type,
        rank=rank,
        world_size=world_size,
        hidden_dim=hidden_dim,
        scale_dim=scale_dim,
        scale_type_size=scale_type_size,
        max_token_type_size=2,
        max_num_inp_token_per_rank=max_num_inp_token_per_rank,
        num_experts_per_rank=num_experts_per_rank,
        num_experts_per_token=num_experts_per_token,
        warp_num_per_block=16,
        block_num=block_num,
        use_external_inp_buf=True,
        use_host_proxy=True,
    )
    benchmark = EpDispatchCombineBenchmark(config, profile_dir)

    with TorchDistContext(rank=rank, world_size=world_size, master_port=port):
        mori.shmem.shmem_torch_process_group_init("default")
        op = mori.ops.EpDispatchCombineOp(config)
        benchmark.run(op)
        # benchmark.stress(op)
        # benchmark.stress_graph(op)
        # benchmark.output()
        # mori.shmem.shmem_finalize()


def bench_dispatch_combine(
    max_num_inp_token_per_rank=4096, block_num=80, profile_dir=None
):
    world_size = 8
    port = get_free_port()
    torch.multiprocessing.spawn(
        _bench_dispatch_combine,
        args=(world_size, port, max_num_inp_token_per_rank, block_num, profile_dir),
        nprocs=world_size,
        join=True,
    )


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Benchmark EP Dispatch Combine")
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=4096,
        help="Maximum number of input tokens per rank (default: 4096)",
    )
    parser.add_argument(
        "--block-num",
        type=int,
        default=80,
        help="The number of block_num (default: 80)",
    )
    parser.add_argument(
        "--profile-dir",
        type=str,
        default=None,
        help="Profile directory (default None)",
    )

    args = parser.parse_args()

    print(f"Running benchmark with max_tokens_per_rank: {args.max_tokens}")
    print("-" * 60)
    bench_dispatch_combine(
        max_num_inp_token_per_rank=args.max_tokens,
        block_num=args.block_num,
        profile_dir=args.profile_dir,
    )
